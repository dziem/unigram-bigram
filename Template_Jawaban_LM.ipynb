{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEMPLATE JAWABAN TUGAS LANGUAGE MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import xlrd\n",
    "import operator\n",
    "import random\n",
    "import math\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset (Training dan Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "book = xlrd.open_workbook('dataTrainArticle.xlsx')\n",
    "book2 = xlrd.open_workbook('dataTestArticle.xlsx')\n",
    "sheet = book.sheet_by_name('Sheet1')\n",
    "sheet2 = book2.sheet_by_name('Sheet1')\n",
    "dataTrainDefault = [[sheet.cell_value(r, c) for c in range(sheet.ncols)] for r in range(sheet.nrows)]\n",
    "dataTestDefault = [[sheet2.cell_value(r, c) for c in range(sheet2.ncols)] for r in range(sheet2.nrows)]\n",
    "#split each sentence in each article\n",
    "dataTrain = [] #init\n",
    "for article in dataTrainDefault:\n",
    "    sentence = article[0].split('.')\n",
    "    for word in sentence:\n",
    "        dataTrain.append(word)\n",
    "dataTest = [] #init\n",
    "counter = 0\n",
    "for article in dataTestDefault:\n",
    "    sentence = article[0].split('.')\n",
    "    for word in sentence:\n",
    "        dataTest.append(word)\n",
    "#add dummy-ish data for OOV (word that doesn't exist in data train), the OOV will be converted to _____\n",
    "#just 4 basic word classes, noun, verbs, adjectives, adverb\n",
    "#without this p(oov) will be 0\n",
    "dataTrain.append('_____ has _____ the _____') \n",
    "#<noun> has <verb> the <noun>\n",
    "dataTrain.append('_____  is one _____ car')\n",
    "#<noun> is one <adjective> car\n",
    "dataTrain.append('_____  is _____ _____ a new car')\n",
    "#<noun> is <adverb> <noun> a new car\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bangun Model Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def buildUnigramModel(trainingSentences):\n",
    "    #init var\n",
    "    wordCount = {} #format wordCount[w(i)] = prob(w(i)), ex wordCount['the'] = 0.454\n",
    "    count = 0\n",
    "    wordCount['<s>'] = 0\n",
    "    wordCount['</s>'] = 0\n",
    "    #count word\n",
    "    for sentence in trainingSentences:\n",
    "        tokens = nltk.word_tokenize(sentence) #break sentence to word\n",
    "        count += 2 #for <s> and </s>\n",
    "        wordCount['<s>'] += 1\n",
    "        wordCount['</s>'] += 1\n",
    "        for token in tokens: #for the actual word\n",
    "            tokenLower = token.lower() #to generalize the letter\n",
    "            if tokenLower in wordCount:\n",
    "                wordCount[tokenLower] += 1\n",
    "            else:\n",
    "                wordCount[tokenLower] = 1\n",
    "            count += 1\n",
    "    unigramModel = wordCount\n",
    "    #count prob\n",
    "    for word in unigramModel:\n",
    "        unigramModel[word] = unigramModel[word] / count\n",
    "    return unigramModel\n",
    "unigramModel = buildUnigramModel(dataTrain)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bangun Model Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def buildBigramModel(trainingSentences):\n",
    "    #init\n",
    "    wordPairCount = {} #format wordPairCount[w(i - 1)][w(i)] = prob(w(i) | w(i - 1)), ex. wordPairCount['nissan']['gt-r'] = 0.38\n",
    "    wordCount = {} #to count the divider for each word\n",
    "    wordCount['<s>'] = 0\n",
    "    wordCount['</s>'] = 0\n",
    "    wordPairCount['<s>'] = {} #there's no wordPairCount['</s>'] because there's nothing after </s>\n",
    "    #count word\n",
    "    for sentence in trainingSentences:\n",
    "        tokens = nltk.word_tokenize(sentence) #break sentence to word\n",
    "        counter = 0 #to track the position of the word, to deal with <s> and </s>\n",
    "        tokenBefore = '<s>' #before the first word\n",
    "        wordCount['<s>'] += 1\n",
    "        wordCount['</s>'] += 1\n",
    "        for token in tokens:\n",
    "            tokenLower = token.lower() #to generalize the letter\n",
    "            counter += 1\n",
    "            if tokenLower not in wordPairCount:\n",
    "                wordPairCount[tokenLower] = {}\n",
    "            if counter == len(tokens): #if word at the end of sentence\n",
    "                if tokenLower in wordPairCount[tokenBefore]: #deal with the word and the word before\n",
    "                    wordPairCount[tokenBefore][tokenLower] += 1\n",
    "                else:\n",
    "                    wordPairCount[tokenBefore][tokenLower] = 1\n",
    "                if '</s>' in wordPairCount[tokenLower]: #deal with the word and </s> after the word\n",
    "                    wordPairCount[tokenLower]['</s>'] += 1\n",
    "                else:\n",
    "                    wordPairCount[tokenLower]['</s>'] = 1\n",
    "            else:\n",
    "                if tokenLower in wordPairCount[tokenBefore]: #deal with the word and the word before\n",
    "                    wordPairCount[tokenBefore][tokenLower] += 1\n",
    "                else:\n",
    "                    wordPairCount[tokenBefore][tokenLower] = 1\n",
    "            tokenBefore = tokenLower #save the word before\n",
    "            if tokenLower in wordCount: #count each word\n",
    "                wordCount[tokenLower] += 1\n",
    "            else:\n",
    "                wordCount[tokenLower] = 1\n",
    "    bigramModel = {}  #init too\n",
    "    #filling the rest of the model count with 0, so that the \"index error\" error won't happen\n",
    "    for wordBefore in wordPairCount.keys():\n",
    "        for theWord in wordCount.keys():\n",
    "            if theWord != '<s>': #because <s> is the first, no word before\n",
    "                if theWord not in wordPairCount[wordBefore]:\n",
    "                    wordPairCount[wordBefore][theWord] = 0\n",
    "    #count prob\n",
    "    for wordBefore in wordPairCount.keys():\n",
    "        bigramModel[wordBefore] = {}\n",
    "        for theWord in wordCount.keys():\n",
    "            if theWord != '<s>': #because <s> is the first, no word before\n",
    "                bigramModel[wordBefore][theWord] = wordPairCount[wordBefore][theWord] / wordCount[theWord]\n",
    "    return bigramModel\n",
    "bigramModel = buildBigramModel(dataTrain)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bangun Fungsi Kata Berikutnya Dengan Probability Tertinggi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting\n"
     ]
    }
   ],
   "source": [
    "#Diberikan sebuah kata, cari kata berikutnya yang memiliki probability tertinggi berdasarkan model bigram\n",
    "#proportional random from top n word\n",
    "#param model, a word, n (top n word)\n",
    "def nextBestWord(bigramModel, currentWord, topn):\n",
    "    #init\n",
    "    top = []\n",
    "    holder = {}\n",
    "    currentWordLow = currentWord.lower() #to make the case same with the model\n",
    "    if currentWordLow not in bigramModel: #for OOV word (that is not in the training/model)\n",
    "        currentWordLow = \"_____\"\n",
    "    for i in range(topn):\n",
    "        maxs = max(bigramModel[currentWordLow].items(), key=operator.itemgetter(1))[0] #get max prob\n",
    "        holder[maxs] = bigramModel[currentWordLow][maxs] #save the max prob\n",
    "        top.append(maxs) #save the max word\n",
    "        bigramModel[currentWordLow].pop(maxs) #pop the max word, so that the next loop get the second max\n",
    "    for i in range(topn): #put the popped back to the model\n",
    "        bigramModel[currentWordLow].update( {top[i] : holder[top[i]]} )\n",
    "    #proportional random\n",
    "    maks = sum(holder.values())\n",
    "    pick = random.uniform(0, maks)\n",
    "    current = 0\n",
    "    for key, value in holder.items():\n",
    "        current += value\n",
    "        if current > pick:\n",
    "            return key\n",
    "    #return nextWord\n",
    "#Lakukan percobaan dengan memberikan sebuah kata, lalu print kata berikutnya\n",
    "print(nextBestWord(bigramModel,\"porsche\",3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hitung probabilitas bigram menggunakan metode interpolasi (interpolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the p(w(i) | w(i - 1)) using the simple interpolation\n",
    "#params unigram model, bigram model, lambda unigram, lambda bigram, w(i - 1), w(i)\n",
    "def countInterpolationBigram(unigramModel, bigramModel, lambdauni, lambdabi, wordBefore, theWord):\n",
    "    if bigramModel[wordBefore][theWord] == 0: #because math.log2(0) crash the program\n",
    "        return (lambdabi * 0) + (lambdauni * math.log2(unigramModel[theWord]))\n",
    "    else:\n",
    "        return (lambdabi * math.log2(bigramModel[wordBefore][theWord])) + (lambdauni * math.log2(unigramModel[theWord]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hitung Perplexity Unigram dan Bigram terhadap Data Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unigram': 702.8664484694253, 'bigram': 12.562666753706731}\n"
     ]
    }
   ],
   "source": [
    "def countPerplexity(unigramModel, bigramModel, testSentences):\n",
    "    perplexity = {} #init\n",
    "    #unigram\n",
    "    count1 = 0 #the M\n",
    "    total1 = 0 #the l\n",
    "    for sentence in testSentences:\n",
    "        pos = 0 #track word position\n",
    "        tokens = nltk.word_tokenize(sentence) #break sentence to words\n",
    "        for word in tokens:\n",
    "            pos += 1\n",
    "            count1 += 1 #for the word\n",
    "            wordLow = word.lower() #to make the case same with the model\n",
    "            if word not in unigramModel: #for OOV word (that is not in the training/model)\n",
    "                wordLow = \"_____\"\n",
    "            if pos > 1:\n",
    "                total1 += math.log2(unigramModel[wordLow])\n",
    "            elif pos == len(tokens):\n",
    "                total1 += math.log2(unigramModel[wordLow])\n",
    "                total1 += math.log2(unigramModel['</s>']) #end of sentence\n",
    "                count1 += 1 #because </s>\n",
    "            elif pos == 1:\n",
    "                count1 += 1 #because <s>\n",
    "                total1 += math.log2(unigramModel['<s>']) #start of sentence\n",
    "                total1 += math.log2(unigramModel[wordLow])\n",
    "    total1 = total1 / count1 #total * 1 / M\n",
    "    total1 = total1 * -1 #negative total\n",
    "    perplexity1 = 2 ** total1 #2^-l\n",
    "    perplexity.update({'unigram' : perplexity1})\n",
    "    #bigram with simple interpolation (one of the smoothing method)\n",
    "    lambdauni = 0.2 #lambda unigram model\n",
    "    lambdabi = 0.8 #lambda bigram model\n",
    "    count2 = 0 #the M\n",
    "    total2 = 0 #the l\n",
    "    for sentence in testSentences:\n",
    "        pos = 0 #track position\n",
    "        tokens = nltk.word_tokenize(sentence) #break sentence to words\n",
    "        wordBefore = '<s>'\n",
    "        for word in tokens:\n",
    "            count2 += 1\n",
    "            pos += 1\n",
    "            wordLow = word.lower() #to make the case same with the model\n",
    "            if wordLow not in bigramModel or wordLow not in unigramModel:\n",
    "                wordLow = \"_____\" #for OOV word (that is not in the training/model)\n",
    "            if pos > 0:\n",
    "                total2 += countInterpolationBigram(unigramModel, bigramModel, lambdauni, lambdabi, wordBefore, wordLow)\n",
    "            elif pos == len(tokens):\n",
    "                total2 += countInterpolationBigram(unigramModel, bigramModel, lambdauni, lambdabi, wordBefore, wordLow)\n",
    "                total2 += countInterpolationBigram(unigramModel, bigramModel, lambdauni, lambdabi, wordLow, '</s>') #end of sentence\n",
    "                count2 += 1 #for end of sentence (</s>)\n",
    "            wordBefore = wordLow #save the w(i - 1)\n",
    "    total2 = total2 / count2 #total * 1 / M\n",
    "    total2 = total2 * -1 #negative total\n",
    "    perplexity2 = 2 ** total2 #2^-l\n",
    "    perplexity.update({'bigram' : perplexity2})\n",
    "    return perplexity\n",
    "print(countPerplexity(unigramModel, bigramModel, dataTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bangun Fungsi Generate Kalimat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' thankfully, knocking 150kg off hitting 500kmh at 277mph over 1000bhp/tonne. has wonderful balance but butter-smooth roads will evaporate in 275/35 tyres moulded components such demands for conversion. has whipped up 258bhp at 6000rpm and 630lb ft torque split. has accidentally made possible without losing the puzzle is 2000ps(w10) boasting faster shifts, clutch and 187,000(ss) magneto rheological suspension geometry with circular daytime running light remap to €68 this scenario. has acquired the wake of dieselgate and admire how awesome it 150bhp less controversial bonnet lives a standalone entity formed'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#params, the model, number of word, and top-n for the generator\n",
    "def generateSentence(model, length, top):\n",
    "    sentence = '' #init\n",
    "    wordBefore = '<s>' #very first word\n",
    "    for i in range(length):\n",
    "        while(True): #to avoid generating _____ (the OOV dummy model)\n",
    "            nextWord = nextBestWord(model,wordBefore,top)\n",
    "            if nextWord != '_____':\n",
    "                break\n",
    "        if nextWord == '<s>':\n",
    "            sentence += ' '\n",
    "        elif nextWord == \"</s>\":\n",
    "            sentence += '.'\n",
    "        else:\n",
    "            #for better spacing\n",
    "            if nextWord != \"'\" and nextWord != '’' and nextWord != '‘' and wordBefore != '’' and wordBefore != '”' and wordBefore != '(' and nextWord != '”' and nextWord != '\"' and nextWord != ',' and nextWord != '(' and nextWord != ')':\n",
    "                sentence += ' '\n",
    "            sentence += nextWord\n",
    "        wordBefore = nextWord\n",
    "    return sentence\n",
    "generateSentence(bigramModel, 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEMPLATE JAWABAN TUGAS LANGUAGE MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import xlrd\n",
    "import operator\n",
    "import random\n",
    "import math\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset (Training dan Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "book = xlrd.open_workbook('dataTrainArticle.xlsx')\n",
    "book2 = xlrd.open_workbook('dataTestArticle.xlsx')\n",
    "sheet = book.sheet_by_name('Sheet1')\n",
    "sheet2 = book2.sheet_by_name('Sheet1')\n",
    "dataTrain = [[sheet.cell_value(r, c) for c in range(sheet.ncols)] for r in range(sheet.nrows)]\n",
    "dataTest = [[sheet2.cell_value(r, c) for c in range(sheet2.ncols)] for r in range(sheet2.nrows)]\n",
    "#add dummy-ish data for OOV (word that doesn't exist in data train), the OOV will be converted to _____\n",
    "#just 4 basic word classes, noun, verbs, adjectives, adverb\n",
    "#without this p(oov) will be 0\n",
    "dataTrain.append(['_____ has _____ the _____']) \n",
    "#<noun> has <verb> the <noun>\n",
    "dataTrain.append(['_____  is one _____ car'])\n",
    "#<noun> is one <adjective> car\n",
    "dataTrain.append(['_____  is _____ _____ a new car'])\n",
    "#<noun> is <adverb> <noun> a new car\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bangun Model Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def buildUnigramModel(trainingSentences):\n",
    "    #init var\n",
    "    wordCount = {} #format wordCount[w(i)] = prob(w(i)), ex wordCount['the'] = 0.454\n",
    "    count = 0\n",
    "    wordCount['<s>'] = 0\n",
    "    wordCount['</s>'] = 0\n",
    "    #count word\n",
    "    for sentence in trainingSentences:\n",
    "        tokens = nltk.word_tokenize(sentence[0]) #break article to word\n",
    "        for token in tokens: #for the actual word\n",
    "            if token == '.':\n",
    "                wordCount['<s>'] += 1\n",
    "                wordCount['</s>'] += 1\n",
    "                count += 2\n",
    "            else:\n",
    "                tokenLower = token.lower() #to generalize the letter\n",
    "                if tokenLower in wordCount:\n",
    "                    wordCount[tokenLower] += 1\n",
    "                else:\n",
    "                    wordCount[tokenLower] = 1\n",
    "                count += 1\n",
    "    unigramModel = wordCount\n",
    "    #count prob\n",
    "    for word in unigramModel:\n",
    "        unigramModel[word] = unigramModel[word] / count\n",
    "    return unigramModel\n",
    "unigramModel = buildUnigramModel(dataTrain)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bangun Model Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def buildBigramModel(trainingSentences):\n",
    "    #init\n",
    "    wordPairCount = {} #format wordPairCount[w(i - 1)][w(i)] = prob(w(i) | w(i - 1)), ex. wordPairCount['nissan']['gt-r'] = 0.38\n",
    "    wordCount = {} #to count the divider for each word\n",
    "    wordCount['<s>'] = 0\n",
    "    wordCount['</s>'] = 0\n",
    "    wordPairCount['<s>'] = {} #there's no wordPairCount['</s>'] because there's nothing after </s>\n",
    "    #count word\n",
    "    for sentence in trainingSentences:\n",
    "        tokens = nltk.word_tokenize(sentence[0]) #break sentence to word\n",
    "        tokenBefore = '<s>' #for the very first word in the article\n",
    "        for token in tokens:\n",
    "            if token == '.':\n",
    "                if '</s>' in wordPairCount[tokenBefore]:\n",
    "                    wordPairCount[tokenBefore]['</s>'] += 1\n",
    "                else:\n",
    "                    wordPairCount[tokenBefore]['</s>'] = 1\n",
    "                tokenBefore = '<s>'\n",
    "                wordCount['<s>'] += 1\n",
    "                wordCount['</s>'] += 1\n",
    "            else:\n",
    "                tokenLower = token.lower() #to generalize the letter\n",
    "                if tokenLower not in wordPairCount: #to save any word to the model as the word before\n",
    "                    wordPairCount[tokenLower] = {}\n",
    "                if tokenLower in wordPairCount[tokenBefore]: #to count the word based on word before\n",
    "                    wordPairCount[tokenBefore][tokenLower] += 1\n",
    "                else:\n",
    "                    wordPairCount[tokenBefore][tokenLower] = 1\n",
    "                tokenBefore = tokenLower #save the word before\n",
    "                if tokenLower in wordCount: #count each word\n",
    "                    wordCount[tokenLower] += 1\n",
    "                else:\n",
    "                    wordCount[tokenLower] = 1\n",
    "    bigramModel = {}  #init too\n",
    "    #filling the rest of the model count with 0, so that the \"index error\" error won't happen\n",
    "    #and also count the probability\n",
    "    for wordBefore in wordPairCount.keys():\n",
    "        bigramModel[wordBefore] = {}\n",
    "        for theWord in wordCount.keys():\n",
    "            if theWord != '<s>': #because <s> is the first, no word before\n",
    "                if theWord not in wordPairCount[wordBefore]:\n",
    "                    wordPairCount[wordBefore][theWord] = 0\n",
    "                bigramModel[wordBefore][theWord] = wordPairCount[wordBefore][theWord] / wordCount[wordBefore]\n",
    "    return bigramModel\n",
    "bigramModel = buildBigramModel(dataTrain)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bangun Fungsi Kata Berikutnya Dengan Probability Tertinggi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cayenne\n"
     ]
    }
   ],
   "source": [
    "#Diberikan sebuah kata, cari kata berikutnya yang memiliki probability tertinggi berdasarkan model bigram\n",
    "#proportional random from top n word\n",
    "#param model, a word, n (top n word)\n",
    "def nextBestWord(bigramModel, currentWord, topn):\n",
    "    #init\n",
    "    top = []\n",
    "    holder = {}\n",
    "    currentWordLow = currentWord.lower() #to make the case same with the model\n",
    "    if currentWordLow not in bigramModel: #for OOV word (that is not in the training/model)\n",
    "        currentWordLow = \"_____\"\n",
    "    for i in range(topn):\n",
    "        maxs = max(bigramModel[currentWordLow].items(), key=operator.itemgetter(1))[0] #get max prob\n",
    "        holder[maxs] = bigramModel[currentWordLow][maxs] #save the max prob\n",
    "        top.append(maxs) #save the max word\n",
    "        bigramModel[currentWordLow].pop(maxs) #pop the max word, so that the next loop get the second max\n",
    "    for i in range(topn): #put the popped back to the model\n",
    "        bigramModel[currentWordLow].update( {top[i] : holder[top[i]]} )\n",
    "    #proportional random\n",
    "    maks = sum(holder.values())\n",
    "    pick = random.uniform(0, maks)\n",
    "    current = 0\n",
    "    for key, value in holder.items():\n",
    "        current += value\n",
    "        if current > pick:\n",
    "            return key\n",
    "#Lakukan percobaan dengan memberikan sebuah kata, lalu print kata berikutnya\n",
    "print(nextBestWord(bigramModel,\"porsche\",3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hitung probabilitas bigram menggunakan metode interpolasi (interpolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the p(w(i) | w(i - 1)) using the simple interpolation\n",
    "#params unigram model, bigram model, lambda unigram, lambda bigram, w(i - 1), w(i)\n",
    "def countInterpolationBigram(unigramModel, bigramModel, lambdauni, lambdabi, wordBefore, theWord):\n",
    "    if bigramModel[wordBefore][theWord] == 0: #because math.log2(0) crash the program\n",
    "        return (lambdabi * 0) + (lambdauni * math.log2(unigramModel[theWord]))\n",
    "    else:\n",
    "        return (lambdabi * math.log2(bigramModel[wordBefore][theWord])) + (lambdauni * math.log2(unigramModel[theWord]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hitung Perplexity Unigram dan Bigram terhadap Data Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unigram': 746.8009860634719, 'bigram': 56.707027552706975}\n"
     ]
    }
   ],
   "source": [
    "def countPerplexity(unigramModel, bigramModel, testSentences):\n",
    "    perplexity = {} #init\n",
    "    #unigram\n",
    "    count1 = 0 #the M\n",
    "    total1 = 0 #the l\n",
    "    for sentence in testSentences:\n",
    "        tokens = nltk.word_tokenize(sentence[0]) #break sentence to words\n",
    "        for word in tokens:\n",
    "            if word == '.':\n",
    "                '''we need to include the begin- and end-sentence markers <s> and\n",
    "                </s> in the probability computation. We also need to include the end-of-sentence\n",
    "                marker </s> (but not the beginning-of-sentence marker <s>) in the total count of\n",
    "                word tokens N. (from jurafsky's book)'''\n",
    "                total1 += math.log2(unigramModel['</s>'])\n",
    "                total1 += math.log2(unigramModel['<s>'])\n",
    "                count1 += 1\n",
    "            else:\n",
    "                wordLow = word.lower() #to make the case same with the model\n",
    "                if wordLow not in unigramModel: #for OOV word (that is not in the training/model)\n",
    "                    wordLow = \"_____\"\n",
    "                total1 += math.log2(unigramModel[wordLow])\n",
    "                count1 += 1 #for the word\n",
    "    total1 = total1 / count1 #total * 1 / M\n",
    "    total1 = total1 * -1 #negative total\n",
    "    perplexity1 = 2 ** total1 #2^-l\n",
    "    perplexity.update({'unigram' : perplexity1})\n",
    "    #bigram with simple interpolation (one of the smoothing method)\n",
    "    lambdauni = 0.5 #lambda unigram model\n",
    "    lambdabi = 0.5 #lambda bigram model\n",
    "    count2 = 0 #the M\n",
    "    total2 = 0 #the l\n",
    "    for sentence in testSentences:\n",
    "        tokens = nltk.word_tokenize(sentence[0]) #break sentence to words\n",
    "        wordBefore = '<s>'\n",
    "        for word in tokens:\n",
    "            count2 += 1\n",
    "            if word == '.':\n",
    "                total2 += countInterpolationBigram(unigramModel, bigramModel, lambdauni, lambdabi, wordBefore, '</s>') #end of sentence\n",
    "                wordBefore = '<s>'\n",
    "            else:\n",
    "                wordLow = word.lower() #to make the case same with the model\n",
    "                if wordLow not in bigramModel or wordLow not in unigramModel:\n",
    "                    wordLow = \"_____\" #for OOV word (that is not in the training/model)\n",
    "                total2 += countInterpolationBigram(unigramModel, bigramModel, lambdauni, lambdabi, wordBefore, wordLow)\n",
    "                wordBefore = wordLow #save the w(i - 1)\n",
    "    total2 = total2 / count2 #total * 1 / M\n",
    "    total2 = total2 * -1 #negative total\n",
    "    perplexity2 = 2 ** total2 #2^-l\n",
    "    perplexity.update({'bigram' : perplexity2})\n",
    "    return perplexity\n",
    "print(countPerplexity(unigramModel, bigramModel, dataTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bangun Fungsi Generate Kalimat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the original. has been mooted. a whole shebang will be built into the most obvious lack of cars like any surprises. a ‘siblings. is all that could drop a car, but now. the car called the company has just look grey body roll'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#params, the model, number of word, and top-n for the generator\n",
    "def generateSentence(model, length, top):\n",
    "    sentence = '' #init\n",
    "    wordBefore = '<s>' #very first word\n",
    "    for i in range(length):\n",
    "        while(True): #to avoid generating _____ (the OOV dummy model)\n",
    "            nextWord = nextBestWord(model,wordBefore,top)\n",
    "            if nextWord != '_____':\n",
    "                break\n",
    "        if nextWord == '<s>':\n",
    "            sentence += ' '\n",
    "        elif nextWord == \"</s>\":\n",
    "            sentence += '.'\n",
    "        else:\n",
    "            #for better spacing\n",
    "            if nextWord != \"'\" and nextWord != '’' and wordBefore != '‘' and wordBefore != '’' and wordBefore != '“' and wordBefore != '(' and nextWord != '”' and nextWord != '\"' and nextWord != ',' and nextWord != '(' and nextWord != ')':\n",
    "                sentence += ' '\n",
    "            sentence += nextWord\n",
    "        wordBefore = nextWord\n",
    "    return sentence\n",
    "generateSentence(bigramModel, 50, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
